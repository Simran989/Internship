{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c2d4ec9",
   "metadata": {},
   "source": [
    "# Web Scraping Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd47ffe1",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53e7b3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all liabraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c8c6fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\USER\\Downloads\\chromedriver_win32 (3)\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2ac157",
   "metadata": {},
   "source": [
    "Scrape the details of most viewed videos on YouTube from Wikipedia.\n",
    "Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\n",
    "\n",
    "You need to find following details:\n",
    "\n",
    "A) Rank B) Name C) Artist D) Upload date E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6923fe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos'\n",
    "driver.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5e3fae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank=[]\n",
    "name=[]\n",
    "artist=[]\n",
    "views=[]\n",
    "upload=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58efaab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all liabraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f88e1bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Views(billions)</th>\n",
       "      <th>Upload Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Baby Shark Dance\"[3]</td>\n",
       "      <td>Pinkfong Baby Shark - Kids' Songs &amp; Stories</td>\n",
       "      <td>7,046,700,000</td>\n",
       "      <td>June 17, 2016</td>\n",
       "      <td>November 2, 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Despacito\"[6]</td>\n",
       "      <td>Luis Fonsi</td>\n",
       "      <td>2,993,700,000</td>\n",
       "      <td>January 12, 2017</td>\n",
       "      <td>August 4, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"See You Again\"[16]</td>\n",
       "      <td>Wiz Khalifa</td>\n",
       "      <td>2,894,000,000</td>\n",
       "      <td>April 6, 2015</td>\n",
       "      <td>July 10, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Gangnam Style\"⁂[24]</td>\n",
       "      <td>Psy</td>\n",
       "      <td>803,700,000</td>\n",
       "      <td>July 15, 2012</td>\n",
       "      <td>November 24, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Baby\"*[61]</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>245,400,000</td>\n",
       "      <td>February 19, 2010</td>\n",
       "      <td>July 16, 2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"Bad Romance\"[65]</td>\n",
       "      <td>Lady Gaga</td>\n",
       "      <td>178,400,000</td>\n",
       "      <td>November 24, 2009</td>\n",
       "      <td>April 14, 2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"Charlie Bit My Finger\"‡[69]</td>\n",
       "      <td>HDCYT</td>\n",
       "      <td>128,900,000</td>\n",
       "      <td>May 22, 2007</td>\n",
       "      <td>October 25, 2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"Evolution of Dance\"[72]</td>\n",
       "      <td>Judson Laipply</td>\n",
       "      <td>118,900,000</td>\n",
       "      <td>April 6, 2006</td>\n",
       "      <td>May 2, 2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"Girlfriend\"‡[74][75]</td>\n",
       "      <td>RCA Records</td>\n",
       "      <td>92,600,000</td>\n",
       "      <td>February 27, 2007</td>\n",
       "      <td>July 17, 2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"Evolution of Dance\"[72]</td>\n",
       "      <td>Judson Laipply</td>\n",
       "      <td>78,400,000</td>\n",
       "      <td>April 6, 2006</td>\n",
       "      <td>March 15, 2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>\"Music Is My Hot Hot Sex\"‡[79]</td>\n",
       "      <td>CLARUSBARTEL72</td>\n",
       "      <td>76,600,000</td>\n",
       "      <td>April 9, 2007</td>\n",
       "      <td>March 1, 2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>\"Evolution of Dance\"*[72]</td>\n",
       "      <td>Judson Laipply</td>\n",
       "      <td>10,600,000</td>\n",
       "      <td>April 6, 2006</td>\n",
       "      <td>May 19, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>\"Pokémon Theme Music Video\"‡[85]</td>\n",
       "      <td>Smosh</td>\n",
       "      <td>4,300,000</td>\n",
       "      <td>November 28, 2005</td>\n",
       "      <td>March 12, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>\"Myspace – The Movie\"‡[90][91]</td>\n",
       "      <td>eggtea</td>\n",
       "      <td>2,700,000</td>\n",
       "      <td>January 31, 2006</td>\n",
       "      <td>February 18, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>\"Phony Photo Booth\"‡[94]</td>\n",
       "      <td>mugenized</td>\n",
       "      <td>3,400,000</td>\n",
       "      <td>December 1, 2005</td>\n",
       "      <td>January 21, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>\"The Chronic of Narnia Rap\"‡[96]</td>\n",
       "      <td>youtubedude</td>\n",
       "      <td>2,300,000</td>\n",
       "      <td>December 18, 2005</td>\n",
       "      <td>January 9, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>\"Ronaldinho: Touch of Gold\"‡*[98]</td>\n",
       "      <td>Nikesoccer</td>\n",
       "      <td>255,000</td>\n",
       "      <td>October 21, 2005</td>\n",
       "      <td>October 31, 2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>\"I/O Brush\"‡*[101]</td>\n",
       "      <td>larfus</td>\n",
       "      <td>247,000</td>\n",
       "      <td>October 5, 2005</td>\n",
       "      <td>October 29, 2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>\"Me at the zoo\"[103]</td>\n",
       "      <td>jawed</td>\n",
       "      <td>1</td>\n",
       "      <td>April 23, 2005</td>\n",
       "      <td>April 23, 2005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Rank  \\\n",
       "0               \"Baby Shark Dance\"[3]   \n",
       "1                      \"Despacito\"[6]   \n",
       "2                 \"See You Again\"[16]   \n",
       "3                \"Gangnam Style\"⁂[24]   \n",
       "4                         \"Baby\"*[61]   \n",
       "5                   \"Bad Romance\"[65]   \n",
       "6        \"Charlie Bit My Finger\"‡[69]   \n",
       "7            \"Evolution of Dance\"[72]   \n",
       "8               \"Girlfriend\"‡[74][75]   \n",
       "9            \"Evolution of Dance\"[72]   \n",
       "10     \"Music Is My Hot Hot Sex\"‡[79]   \n",
       "11          \"Evolution of Dance\"*[72]   \n",
       "12   \"Pokémon Theme Music Video\"‡[85]   \n",
       "13     \"Myspace – The Movie\"‡[90][91]   \n",
       "14           \"Phony Photo Booth\"‡[94]   \n",
       "15   \"The Chronic of Narnia Rap\"‡[96]   \n",
       "16  \"Ronaldinho: Touch of Gold\"‡*[98]   \n",
       "17                 \"I/O Brush\"‡*[101]   \n",
       "18               \"Me at the zoo\"[103]   \n",
       "\n",
       "                                           Name         Artist  \\\n",
       "0   Pinkfong Baby Shark - Kids' Songs & Stories  7,046,700,000   \n",
       "1                                    Luis Fonsi  2,993,700,000   \n",
       "2                                   Wiz Khalifa  2,894,000,000   \n",
       "3                                           Psy    803,700,000   \n",
       "4                                 Justin Bieber    245,400,000   \n",
       "5                                     Lady Gaga    178,400,000   \n",
       "6                                         HDCYT    128,900,000   \n",
       "7                                Judson Laipply    118,900,000   \n",
       "8                                   RCA Records     92,600,000   \n",
       "9                                Judson Laipply     78,400,000   \n",
       "10                               CLARUSBARTEL72     76,600,000   \n",
       "11                               Judson Laipply     10,600,000   \n",
       "12                                        Smosh      4,300,000   \n",
       "13                                       eggtea      2,700,000   \n",
       "14                                    mugenized      3,400,000   \n",
       "15                                  youtubedude      2,300,000   \n",
       "16                                   Nikesoccer        255,000   \n",
       "17                                       larfus        247,000   \n",
       "18                                        jawed              1   \n",
       "\n",
       "      Views(billions)        Upload Date  \n",
       "0       June 17, 2016   November 2, 2020  \n",
       "1    January 12, 2017     August 4, 2017  \n",
       "2       April 6, 2015      July 10, 2017  \n",
       "3       July 15, 2012  November 24, 2012  \n",
       "4   February 19, 2010      July 16, 2010  \n",
       "5   November 24, 2009     April 14, 2010  \n",
       "6        May 22, 2007   October 25, 2009  \n",
       "7       April 6, 2006        May 2, 2009  \n",
       "8   February 27, 2007      July 17, 2008  \n",
       "9       April 6, 2006     March 15, 2008  \n",
       "10      April 9, 2007      March 1, 2008  \n",
       "11      April 6, 2006       May 19, 2006  \n",
       "12  November 28, 2005     March 12, 2006  \n",
       "13   January 31, 2006  February 18, 2006  \n",
       "14   December 1, 2005   January 21, 2006  \n",
       "15  December 18, 2005    January 9, 2006  \n",
       "16   October 21, 2005   October 31, 2005  \n",
       "17    October 5, 2005   October 29, 2005  \n",
       "18     April 23, 2005     April 23, 2005  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in driver.find_elements(By.XPATH,'//table[3]//tbody//tr//td[1]'):\n",
    "    rank.append(i.text)\n",
    "    \n",
    "for i in driver.find_elements(By.XPATH,'//table[3]//tbody//tr//td[2]'):\n",
    "    name.append(i.text)\n",
    "    \n",
    "for i in driver.find_elements(By.XPATH,'//table[3]//tbody//tr//td[3]'):\n",
    "    artist.append(i.text)\n",
    "    \n",
    "for i in driver.find_elements(By.XPATH,'//table[3]//tbody//tr//td[4]'):\n",
    "    views.append(i.text)\n",
    "    \n",
    "for i in driver.find_elements(By.XPATH,'//table[3]//tbody//tr//td[5]'):\n",
    "    upload.append(i.text)\n",
    "    \n",
    "    \n",
    "que1=pd.DataFrame({'Rank':rank,'Name':name,'Artist':artist,'Views(billions)':views,'Upload Date':upload})\n",
    "que1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b4e0a4",
   "metadata": {},
   "source": [
    "2. Scrape the details team India’s international fixtures from bcci.tv. Url = https://www.bcci.tv/.\n",
    "You need to find following details: A) Match title (I.e. 1st ODI) B) Series C) Place D) Date E) Time Note: - From bcci.tv home page you have reach to the international fixture page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f857fa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.bcci.tv/'\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acfc4020",
   "metadata": {},
   "outputs": [],
   "source": [
    "intn=driver.find_element(By.XPATH,'/html/body/nav/div/div[2]/ul[1]/li[2]/a')\n",
    "intn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab3872f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Match Title</th>\n",
       "      <th>Series</th>\n",
       "      <th>Place</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5th T20I -</td>\n",
       "      <td>ICC MENS T20 WORLD CUP 2022</td>\n",
       "      <td>Melbourne Cricket Ground, Melbourne</td>\n",
       "      <td>6 NOV 2022</td>\n",
       "      <td>1:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1st T20I -</td>\n",
       "      <td>INDIA TOUR OF NEW ZEALAND T20 SERIES 2022-23</td>\n",
       "      <td>Sky Stadium, Wellington</td>\n",
       "      <td>18 NOV 2022</td>\n",
       "      <td>12:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2nd T20I -</td>\n",
       "      <td>INDIA TOUR OF NEW ZEALAND T20 SERIES 2022-23</td>\n",
       "      <td>Bay Oval, Mount Maunganui</td>\n",
       "      <td>20 NOV 2022</td>\n",
       "      <td>12:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3rd T20I -</td>\n",
       "      <td>INDIA TOUR OF NEW ZEALAND T20 SERIES 2022-23</td>\n",
       "      <td>Mclean Park, Napier</td>\n",
       "      <td>22 NOV 2022</td>\n",
       "      <td>12:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1st ODI -</td>\n",
       "      <td>INDIA TOUR OF NEW ZEALAND ODI SERIES 2022-23</td>\n",
       "      <td>Eden Park, Auckland</td>\n",
       "      <td>25 NOV 2022</td>\n",
       "      <td>7:00 AM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2nd ODI -</td>\n",
       "      <td>INDIA TOUR OF NEW ZEALAND ODI SERIES 2022-23</td>\n",
       "      <td>Seddon Park, Hamilton</td>\n",
       "      <td>27 NOV 2022</td>\n",
       "      <td>7:00 AM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3rd ODI -</td>\n",
       "      <td>INDIA TOUR OF NEW ZEALAND ODI SERIES 2022-23</td>\n",
       "      <td>Hagley Oval, Christchurch</td>\n",
       "      <td>30 NOV 2022</td>\n",
       "      <td>7:00 AM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1st ODI -</td>\n",
       "      <td>INDIA TOUR OF BANGLADESH ODI SERIES 2022-23</td>\n",
       "      <td>Shere Bangla National Stadium, Mirpur, Dhaka</td>\n",
       "      <td>4 DEC 2022</td>\n",
       "      <td>12:30 PM IST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Match Title                                        Series  \\\n",
       "0  5th T20I -                   ICC MENS T20 WORLD CUP 2022   \n",
       "1  1st T20I -  INDIA TOUR OF NEW ZEALAND T20 SERIES 2022-23   \n",
       "2  2nd T20I -  INDIA TOUR OF NEW ZEALAND T20 SERIES 2022-23   \n",
       "3  3rd T20I -  INDIA TOUR OF NEW ZEALAND T20 SERIES 2022-23   \n",
       "4   1st ODI -  INDIA TOUR OF NEW ZEALAND ODI SERIES 2022-23   \n",
       "5   2nd ODI -  INDIA TOUR OF NEW ZEALAND ODI SERIES 2022-23   \n",
       "6   3rd ODI -  INDIA TOUR OF NEW ZEALAND ODI SERIES 2022-23   \n",
       "7   1st ODI -   INDIA TOUR OF BANGLADESH ODI SERIES 2022-23   \n",
       "\n",
       "                                           Place         Date          Time  \n",
       "0            Melbourne Cricket Ground, Melbourne   6 NOV 2022   1:30 PM IST  \n",
       "1                        Sky Stadium, Wellington  18 NOV 2022  12:00 PM IST  \n",
       "2                      Bay Oval, Mount Maunganui  20 NOV 2022  12:00 PM IST  \n",
       "3                            Mclean Park, Napier  22 NOV 2022  12:00 PM IST  \n",
       "4                            Eden Park, Auckland  25 NOV 2022   7:00 AM IST  \n",
       "5                          Seddon Park, Hamilton  27 NOV 2022   7:00 AM IST  \n",
       "6                      Hagley Oval, Christchurch  30 NOV 2022   7:00 AM IST  \n",
       "7   Shere Bangla National Stadium, Mirpur, Dhaka   4 DEC 2022  12:30 PM IST  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title=[]\n",
    "for i in driver.find_elements(By.XPATH,'//span[@class=\"matchOrderText ng-binding ng-scope\"]'):\n",
    "    title.append(i.text)\n",
    "    \n",
    "series=[]\n",
    "for i in driver.find_elements(By.XPATH,'//h5[@class=\"fix-text\"]//span[@class=\"ng-binding\"]'):\n",
    "    series.append(i.text)\n",
    "    \n",
    "place=[]\n",
    "for i in driver.find_elements(By.XPATH,'//div[@class=\"fix-place ng-binding ng-scope\"]'):\n",
    "    place.append(i.text.split(\"-\")[1])\n",
    "    \n",
    "date=[]\n",
    "for i in driver.find_elements(By.XPATH,'//div[@class=\"match-card-left match-schedule\"]//h5'):\n",
    "    date.append(i.text)\n",
    "    \n",
    "time=[]\n",
    "for i in driver.find_elements(By.XPATH,'//div[@class=\"match-card-right match-schedule \"]//h5'):\n",
    "    time.append(i.text)\n",
    "    \n",
    "que2=pd.DataFrame({'Match Title':title,'Series':series,'Place':place,'Date':date,'Time':time})\n",
    "que2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d811d808",
   "metadata": {},
   "source": [
    "3.Scrape the details of selenium exception from guru99.com. Url = https://www.guru99.com/\n",
    "You need to find following details: A) Name B) Description\n",
    "\n",
    "Note: - From guru99 home page you have to reach to selenium exception handling page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a7afbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8d92f8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\USER\\Downloads\\chromedriver_win32 (3)\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cba6cc6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Name, Description]\n",
       "Index: []"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Connecting to the webpage \"https://www.guru99.com/\"\n",
    "\n",
    "url = \"https://www.guru99.com/\"\n",
    "driver.get(url)\n",
    "\n",
    "# Click on selenium in Testing Section\n",
    "\n",
    "click_on_selenium = driver.find_element(By.XPATH,'//*[@id=\"java_technologies\"]/li[3]/a')\n",
    "click_on_selenium.click()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Clic on Selenium Exception Handling\n",
    "\n",
    "selenium_exception = driver.find_element(By.XPATH,'//*[@id=\"post-193\"]/div/div/table[5]/tbody/tr[34]/td[1]/a')\n",
    "selenium_exception.click()\n",
    "\n",
    "# Names\n",
    "\n",
    "names = []\n",
    "name = driver.find_elements(By.XPATH,'//table[@class=\"table table-striped\"]//tr//td[1]')\n",
    "for i in name:\n",
    "    names.append(i.text)\n",
    "\n",
    "\n",
    "# Description\n",
    "\n",
    "description = []\n",
    "desc = driver.find_elements(By.XPATH,'//table[@class=\"table table-striped\"]//tr//td[2]')\n",
    "for i in desc:\n",
    "    description.append(i.text)\n",
    "    \n",
    "# Data Frame\n",
    "\n",
    "Details = pd.DataFrame()\n",
    "Details[\"Name\"] = names\n",
    "Details[\"Description\"] = description\n",
    "\n",
    "Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5df057",
   "metadata": {},
   "source": [
    "Question No.4 :\n",
    "Scrape the details of State-wise GDP of India from statisticstime.com.\n",
    "\n",
    "Url = http://statisticstimes.com/\n",
    "\n",
    "You have to find following details:\n",
    "\n",
    "A) Rank\n",
    "\n",
    "B) State\n",
    "\n",
    "C) GSDP at current price (19-20)\n",
    "\n",
    "D) GSDP at current price (18-19)\n",
    "\n",
    "E) Share(18-19)\n",
    "\n",
    "F) GDP($ billion)\n",
    "\n",
    "Note: - From statisticstimes home page you have to reach to economy page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7db5177b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<selenium.webdriver.chrome.webdriver.WebDriver (session=\"ad78edb702a0bb95a5fa83ef782855ee\")>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>State</th>\n",
       "      <th>GSDP at current price (19-20)</th>\n",
       "      <th>GSDP at current price (18-19)</th>\n",
       "      <th>Share(18-19)</th>\n",
       "      <th>GDP($ billion)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>-</td>\n",
       "      <td>2,632,792</td>\n",
       "      <td>13.94%</td>\n",
       "      <td>399.921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>1,845,853</td>\n",
       "      <td>1,630,208</td>\n",
       "      <td>8.63%</td>\n",
       "      <td>247.629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>1,687,818</td>\n",
       "      <td>1,584,764</td>\n",
       "      <td>8.39%</td>\n",
       "      <td>240.726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Gujarat</td>\n",
       "      <td>-</td>\n",
       "      <td>1,502,899</td>\n",
       "      <td>7.96%</td>\n",
       "      <td>228.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>1,631,977</td>\n",
       "      <td>1,493,127</td>\n",
       "      <td>7.91%</td>\n",
       "      <td>226.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>West Bengal</td>\n",
       "      <td>1,253,832</td>\n",
       "      <td>1,089,898</td>\n",
       "      <td>5.77%</td>\n",
       "      <td>165.556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Rajasthan</td>\n",
       "      <td>1,020,989</td>\n",
       "      <td>942,586</td>\n",
       "      <td>4.99%</td>\n",
       "      <td>143.179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>972,782</td>\n",
       "      <td>862,957</td>\n",
       "      <td>4.57%</td>\n",
       "      <td>131.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Telangana</td>\n",
       "      <td>969,604</td>\n",
       "      <td>861,031</td>\n",
       "      <td>4.56%</td>\n",
       "      <td>130.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Madhya Pradesh</td>\n",
       "      <td>906,672</td>\n",
       "      <td>809,592</td>\n",
       "      <td>4.29%</td>\n",
       "      <td>122.977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Kerala</td>\n",
       "      <td>-</td>\n",
       "      <td>781,653</td>\n",
       "      <td>4.14%</td>\n",
       "      <td>118.733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>856,112</td>\n",
       "      <td>774,870</td>\n",
       "      <td>4.10%</td>\n",
       "      <td>117.703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Haryana</td>\n",
       "      <td>831,610</td>\n",
       "      <td>734,163</td>\n",
       "      <td>3.89%</td>\n",
       "      <td>111.519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Bihar</td>\n",
       "      <td>611,804</td>\n",
       "      <td>530,363</td>\n",
       "      <td>2.81%</td>\n",
       "      <td>80.562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Punjab</td>\n",
       "      <td>574,760</td>\n",
       "      <td>526,376</td>\n",
       "      <td>2.79%</td>\n",
       "      <td>79.957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Odisha</td>\n",
       "      <td>521,275</td>\n",
       "      <td>487,805</td>\n",
       "      <td>2.58%</td>\n",
       "      <td>74.098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Assam</td>\n",
       "      <td>-</td>\n",
       "      <td>315,881</td>\n",
       "      <td>1.67%</td>\n",
       "      <td>47.982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Chhattisgarh</td>\n",
       "      <td>329,180</td>\n",
       "      <td>304,063</td>\n",
       "      <td>1.61%</td>\n",
       "      <td>46.187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Jharkhand</td>\n",
       "      <td>328,598</td>\n",
       "      <td>297,204</td>\n",
       "      <td>1.57%</td>\n",
       "      <td>45.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Uttarakhand</td>\n",
       "      <td>-</td>\n",
       "      <td>245,895</td>\n",
       "      <td>1.30%</td>\n",
       "      <td>37.351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Jammu &amp; Kashmir</td>\n",
       "      <td>-</td>\n",
       "      <td>155,956</td>\n",
       "      <td>0.83%</td>\n",
       "      <td>23.690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>Himachal Pradesh</td>\n",
       "      <td>165,472</td>\n",
       "      <td>153,845</td>\n",
       "      <td>0.81%</td>\n",
       "      <td>23.369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Goa</td>\n",
       "      <td>80,449</td>\n",
       "      <td>73,170</td>\n",
       "      <td>0.39%</td>\n",
       "      <td>11.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>Tripura</td>\n",
       "      <td>55,984</td>\n",
       "      <td>49,845</td>\n",
       "      <td>0.26%</td>\n",
       "      <td>7.571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>Chandigarh</td>\n",
       "      <td>-</td>\n",
       "      <td>42,114</td>\n",
       "      <td>0.22%</td>\n",
       "      <td>6.397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>Puducherry</td>\n",
       "      <td>38,253</td>\n",
       "      <td>34,433</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>Meghalaya</td>\n",
       "      <td>36,572</td>\n",
       "      <td>33,481</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Sikkim</td>\n",
       "      <td>32,496</td>\n",
       "      <td>28,723</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>Manipur</td>\n",
       "      <td>31,790</td>\n",
       "      <td>27,870</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>Nagaland</td>\n",
       "      <td>-</td>\n",
       "      <td>27,283</td>\n",
       "      <td>0.14%</td>\n",
       "      <td>4.144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>Arunachal Pradesh</td>\n",
       "      <td>-</td>\n",
       "      <td>24,603</td>\n",
       "      <td>0.13%</td>\n",
       "      <td>3.737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>Mizoram</td>\n",
       "      <td>26,503</td>\n",
       "      <td>22,287</td>\n",
       "      <td>0.12%</td>\n",
       "      <td>3.385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>Andaman &amp; Nicobar Islands</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                      State GSDP at current price (19-20)  \\\n",
       "0     1                Maharashtra                             -   \n",
       "1     2                 Tamil Nadu                     1,845,853   \n",
       "2     3              Uttar Pradesh                     1,687,818   \n",
       "3     4                    Gujarat                             -   \n",
       "4     5                  Karnataka                     1,631,977   \n",
       "5     6                West Bengal                     1,253,832   \n",
       "6     7                  Rajasthan                     1,020,989   \n",
       "7     8             Andhra Pradesh                       972,782   \n",
       "8     9                  Telangana                       969,604   \n",
       "9    10             Madhya Pradesh                       906,672   \n",
       "10   11                     Kerala                             -   \n",
       "11   12                      Delhi                       856,112   \n",
       "12   13                    Haryana                       831,610   \n",
       "13   14                      Bihar                       611,804   \n",
       "14   15                     Punjab                       574,760   \n",
       "15   16                     Odisha                       521,275   \n",
       "16   17                      Assam                             -   \n",
       "17   18               Chhattisgarh                       329,180   \n",
       "18   19                  Jharkhand                       328,598   \n",
       "19   20                Uttarakhand                             -   \n",
       "20   21            Jammu & Kashmir                             -   \n",
       "21   22           Himachal Pradesh                       165,472   \n",
       "22   23                        Goa                        80,449   \n",
       "23   24                    Tripura                        55,984   \n",
       "24   25                 Chandigarh                             -   \n",
       "25   26                 Puducherry                        38,253   \n",
       "26   27                  Meghalaya                        36,572   \n",
       "27   28                     Sikkim                        32,496   \n",
       "28   29                    Manipur                        31,790   \n",
       "29   30                   Nagaland                             -   \n",
       "30   31          Arunachal Pradesh                             -   \n",
       "31   32                    Mizoram                        26,503   \n",
       "32   33  Andaman & Nicobar Islands                             -   \n",
       "\n",
       "   GSDP at current price (18-19) Share(18-19) GDP($ billion)  \n",
       "0                      2,632,792       13.94%        399.921  \n",
       "1                      1,630,208        8.63%        247.629  \n",
       "2                      1,584,764        8.39%        240.726  \n",
       "3                      1,502,899        7.96%        228.290  \n",
       "4                      1,493,127        7.91%        226.806  \n",
       "5                      1,089,898        5.77%        165.556  \n",
       "6                        942,586        4.99%        143.179  \n",
       "7                        862,957        4.57%        131.083  \n",
       "8                        861,031        4.56%        130.791  \n",
       "9                        809,592        4.29%        122.977  \n",
       "10                       781,653        4.14%        118.733  \n",
       "11                       774,870        4.10%        117.703  \n",
       "12                       734,163        3.89%        111.519  \n",
       "13                       530,363        2.81%         80.562  \n",
       "14                       526,376        2.79%         79.957  \n",
       "15                       487,805        2.58%         74.098  \n",
       "16                       315,881        1.67%         47.982  \n",
       "17                       304,063        1.61%         46.187  \n",
       "18                       297,204        1.57%         45.145  \n",
       "19                       245,895        1.30%         37.351  \n",
       "20                       155,956        0.83%         23.690  \n",
       "21                       153,845        0.81%         23.369  \n",
       "22                        73,170        0.39%         11.115  \n",
       "23                        49,845        0.26%          7.571  \n",
       "24                        42,114        0.22%          6.397  \n",
       "25                        34,433        0.18%          5.230  \n",
       "26                        33,481        0.18%          5.086  \n",
       "27                        28,723        0.15%          4.363  \n",
       "28                        27,870        0.15%          4.233  \n",
       "29                        27,283        0.14%          4.144  \n",
       "30                        24,603        0.13%          3.737  \n",
       "31                        22,287        0.12%          3.385  \n",
       "32                             -            -              -  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's import the necessary libraries for scraping\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "# connecting to the browser\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\USER\\Downloads\\chromedriver_win32 (3)\\chromedriver.exe\")\n",
    "print(driver)\n",
    "\n",
    "\n",
    "# open the url www.amazon.in\n",
    "url = \"http://statisticstimes.com/\"\n",
    "driver.get(url)\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Click on India in Economy Section\n",
    "\n",
    "india_link = []\n",
    "india = driver.find_element(By.XPATH,'//div[@class=\"navbar\"]//div[2]//a[3]')\n",
    "india_link.append(india.get_attribute(\"href\"))\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Click on GDP of indian states\n",
    "\n",
    "for i in india_link:\n",
    "    driver.get(i)\n",
    "    india_gdp = driver.find_element(By.XPATH,'//ul[@style=\"list-style-type:none;margin-left:20px;\"]//li[1]//a')\n",
    "    india_gdp.click()\n",
    "    \n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Create empty required lists\n",
    "    \n",
    "    rank = []\n",
    "    state = []\n",
    "    gsdp_19_20 = []\n",
    "    gsdp_18_19 = []\n",
    "    share_18_19 = []\n",
    "    gdp_billion = []\n",
    "    \n",
    "    # Finding the all elements\n",
    "    \n",
    "    rnk = driver.find_elements(By.XPATH,'//table[@id=\"table_id\"]//tbody//tr/td[1]')\n",
    "    stat = driver.find_elements(By.XPATH,'//table[@id=\"table_id\"]//tbody//tr/td[2]')\n",
    "    gsd_19_20 = driver.find_elements(By.XPATH,'//table[@id=\"table_id\"]//tbody//tr/td[3]')\n",
    "    gsd_18_19 = driver.find_elements(By.XPATH,'//table[@id=\"table_id\"]//tbody//tr/td[4]')\n",
    "    share = driver.find_elements(By.XPATH,'//table[@id=\"table_id\"]//tbody//tr/td[5]')\n",
    "    billion = driver.find_elements(By.XPATH,'//table[@id=\"table_id\"]//tbody//tr/td[6]')\n",
    "    \n",
    "    # Filling the all empty lists\n",
    "    \n",
    "    for j in rnk:\n",
    "        rank.append(j.text)\n",
    "    for j in stat:\n",
    "        state.append(j.text)\n",
    "    for j in gsd_19_20:\n",
    "        gsdp_19_20.append(j.text)\n",
    "    for j in gsd_18_19:\n",
    "        gsdp_18_19.append(j.text)\n",
    "    for j in share:\n",
    "        share_18_19.append(j.text)\n",
    "    for j in billion:\n",
    "        gdp_billion.append(j.text)\n",
    "        \n",
    "# Data Frame\n",
    "    \n",
    "State_wise_GDP = pd.DataFrame()\n",
    "State_wise_GDP[\"Rank\"] = rank\n",
    "State_wise_GDP[\"State\"] = state\n",
    "State_wise_GDP[\"GSDP at current price (19-20)\"] = gsdp_19_20\n",
    "State_wise_GDP[\"GSDP at current price (18-19)\"] = gsdp_18_19\n",
    "State_wise_GDP[\"Share(18-19)\"] = share_18_19\n",
    "State_wise_GDP[\"GDP($ billion)\"] = gdp_billion\n",
    "\n",
    "State_wise_GDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c615187d",
   "metadata": {},
   "source": [
    "Question No. 5 :\n",
    "Scrape the details of trending repositories on Github.com.\n",
    "\n",
    "Url = https://github.com/\n",
    "\n",
    "You have to find the following details:\n",
    "\n",
    "A) Repository title\n",
    "\n",
    "B) Repository description\n",
    "\n",
    "C) Contributors count\n",
    "\n",
    "D) Language used\n",
    "\n",
    "Note: - From the home page you have to click on the trending option from Explore menu through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc592f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the necessary libraries for scraping\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "# connecting to the browser\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\USER\\Downloads\\chromedriver_win32 (3)\\chromedriver.exe\")\n",
    "print(driver)\n",
    "\n",
    "\n",
    "# open the url www.amazon.in\n",
    "url = \"https://github.com/\"\n",
    "driver.get(url)\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "# Find the trending link from Explore\n",
    "\n",
    "trending = []\n",
    "tren = driver.find_element(By.XPATH,'//ul[2][@class=\"list-style-none mb-3\"]//li[3]//a')\n",
    "trending.append(tren.get_attribute(\"href\"))\n",
    "\n",
    "# Going to trending option\n",
    "\n",
    "for i in trending:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Create required empty lists\n",
    "    \n",
    "    title = []\n",
    "    description = []\n",
    "    count = []\n",
    "    language = []\n",
    "    \n",
    "    # Finding the all elements\n",
    "    \n",
    "    tit = driver.find_elements(By.XPATH,'//article[@class=\"Box-row\"]//h1')\n",
    "    des = driver.find_elements(By.XPATH,'//article[@class=\"Box-row\"]//p')\n",
    "    cou = driver.find_elements(By.XPATH,'//div[@class=\"f6 color-text-secondary mt-2\"]//span[3]')\n",
    "    lang = driver.find_elements(By.XPATH,'//div[@class=\"f6 color-text-secondary mt-2\"]//span//span[2]')\n",
    "    \n",
    "    # Filling the all empty lists\n",
    "    \n",
    "    for j in tit:\n",
    "        try:\n",
    "            title.append(j.text)\n",
    "        except:\n",
    "            title.append(\"-\")\n",
    "    for j in des:\n",
    "        try:\n",
    "            description.append(j.text)\n",
    "        except:\n",
    "            description.append(\"-\")\n",
    "    for j in cou:\n",
    "        try:\n",
    "            count.append(j.text)\n",
    "        except:\n",
    "            count.append(\"-\")\n",
    "    for j in lang:\n",
    "        try:\n",
    "            language.append(j.text)\n",
    "        except:\n",
    "            language.append(\"-\")\n",
    "            \n",
    "# Data Frame\n",
    "\n",
    "Top_5_Trending_Repositories = pd.DataFrame(index = list(range(1,6)))\n",
    "Top_5_Trending_Repositories[\"Repository Title\"] = title[:5]\n",
    "Top_5_Trending_Repositories[\"Repository Description\"] = description[:5]\n",
    "Top_5_Trending_Repositories[\"Contributors Count\"] = count[:5]\n",
    "Top_5_Trending_Repositories[\"Language Used\"] = language[:5]\n",
    "\n",
    "Top_5_Trending_Repositories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3f5a93",
   "metadata": {},
   "source": [
    "Question No. 6 :\n",
    "Scrape the details of top 100 songs on billboard.com.\n",
    "\n",
    "Url = https://www.billboard.com/\n",
    "\n",
    "You have to find the following details:\n",
    "\n",
    "A) Song name\n",
    "\n",
    "B) Artist name\n",
    "\n",
    "C) Last week rank\n",
    "\n",
    "D) Peak rank\n",
    "\n",
    "E) Weeks on board\n",
    "\n",
    "Note: - From the home page you have to click on the charts option then hot 100-page link through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd8e5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the necessary libraries for scraping\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "# connecting to the browser\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\USER\\Downloads\\chromedriver_win32 (3)\\chromedriver.exe\")\n",
    "print(driver)\n",
    "\n",
    "\n",
    "# open the url www.amazon.in\n",
    "url = \"https://www.billboard.com/\"\n",
    "driver.get(url)\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "# Find the hot-100 link page\n",
    "\n",
    "hot_100 = []\n",
    "hot = driver.find_element(By.XPATH,'//span[@class=\"header__submenu__container\"]//li[1]//a')\n",
    "hot_100.append(hot.get_attribute(\"href\"))\n",
    "\n",
    "# Going to hot-100 link page\n",
    "\n",
    "for i in hot_100:\n",
    "    driver.get(i)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Create required empty lists\n",
    "    \n",
    "    song = []\n",
    "    artist = []\n",
    "    last_weak = []\n",
    "    peak_rank = []\n",
    "    on_board = []\n",
    "    \n",
    "    # Finding the all elements\n",
    "    \n",
    "    sng = driver.find_elements(By.XPATH,'//span[@class=\"chart-element__information__song text--truncate color--primary\"]')\n",
    "    artst = driver.find_elements(By.XPATH,'//span[@class=\"chart-element__information__artist text--truncate color--secondary\"]')\n",
    "    lst_wk = driver.find_elements(By.XPATH,'//div[@class=\"chart-element__meta text--center color--secondary text--last\"]')\n",
    "    pek_rnk = driver.find_elements(By.XPATH,'//div[@class=\"chart-element__meta text--center color--secondary text--peak\"]')\n",
    "    on_brd = driver.find_elements(By.XPATH,'//div[@class=\"chart-element__meta text--center color--secondary text--week\"]')\n",
    "    \n",
    "    # Filling the all empty lists\n",
    "    \n",
    "    for j in sng:\n",
    "        try:\n",
    "            song.append(j.text)\n",
    "        except:\n",
    "            song.append(\"-\")\n",
    "    for j in artst:\n",
    "        try:\n",
    "            artist.append(j.text)\n",
    "        except:\n",
    "            artist.append(\"-\")\n",
    "    for j in lst_wk:\n",
    "        try:\n",
    "            last_weak.append(j.text)\n",
    "        except:\n",
    "            last_weak.append(\"-\")\n",
    "    for j in pek_rnk:\n",
    "        try:\n",
    "            peak_rank.append(j.text)\n",
    "        except:\n",
    "            peak_rank.append(\"-\")\n",
    "    for j in on_brd:\n",
    "        try:\n",
    "            on_board.append(j.text)\n",
    "        except:\n",
    "            on_board.append(\"-\")\n",
    "            \n",
    "# Data Frame\n",
    "\n",
    "Top_100_Songs = pd.DataFrame(index = list(range(1,101)))\n",
    "Top_100_Songs[\"Song Name\"] = song\n",
    "Top_100_Songs[\"Artist Name\"] = artist\n",
    "Top_100_Songs[\"Last Week Rank\"] = last_weak\n",
    "Top_100_Songs[\"Peak Rank\"] = peak_rank\n",
    "Top_100_Songs[\"Weeks on Board\"] = on_board\n",
    "\n",
    "Top_100_Songs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e7803a",
   "metadata": {},
   "source": [
    "Question No. 7 :\n",
    "Scrape the details of Data science recruiters from naukri.com.\n",
    "\n",
    "Url = https://www.naukri.com/\n",
    "\n",
    "You have to find the following details:\n",
    "\n",
    "A) Name\n",
    "\n",
    "B) Designation\n",
    "\n",
    "C) Company\n",
    "\n",
    "D) Skills they hire for\n",
    "\n",
    "E) Location\n",
    "\n",
    "Note: - From naukri.com homepage click on the recruiters option and the on the search pane type Data science and click on search. All this should be done through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4410a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the necessary libraries for scraping\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "# connecting to the browser\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "print(driver)\n",
    "\n",
    "\n",
    "# open the url www.amazon.in\n",
    "url = \"https://www.naukri.com/\"\n",
    "driver.get(url)\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "# Find the Recruiter Link\n",
    "\n",
    "recruiter = []\n",
    "rect = driver.find_element_by_xpath('//a[@title=\"Search Recruiters\"]')\n",
    "recruiter.append(rect.get_attribute(\"href\"))\n",
    "\n",
    "for i in recruiter:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Sending the required keys to search box\n",
    "    keys = driver.find_element_by_xpath('//input[@class=\"sugInp\"]').send_keys(\"Data Science\")\n",
    "\n",
    "    # Click on Search Button\n",
    "    driver.find_element_by_xpath('//button[@class=\"fl qsbSrch blueBtn\"]').click()\n",
    "    \n",
    "    # Create required empty lists\n",
    "    \n",
    "    name = []\n",
    "    designation = []\n",
    "    company = []\n",
    "    skills = []\n",
    "    location = []\n",
    "    \n",
    "    # Finding the all elements\n",
    "    \n",
    "    nam = driver.find_elements_by_xpath('//span[@class=\"fl ellipsis\"]')\n",
    "    des = driver.find_elements_by_xpath('//span[@class=\"ellipsis clr\"]')\n",
    "    com = driver.find_elements_by_xpath('//a[2][@class=\"ellipsis\"]')\n",
    "    skil = driver.find_elements_by_xpath('//div[@class=\"hireSec highlightable\"]')\n",
    "    loc = driver.find_elements_by_xpath('//p[@class=\"highlightable\"]//small')\n",
    "    \n",
    "    # Filling the all empty lists\n",
    "    \n",
    "    for j in nam:\n",
    "        try:\n",
    "            name.append(j.text)\n",
    "        except:\n",
    "            name.append(\"-\")\n",
    "    for j in des:\n",
    "        try:\n",
    "            designation.append(j.text)\n",
    "        except:\n",
    "            designation.append(\"-\")\n",
    "    for j in com:\n",
    "        try:\n",
    "            company.append(j.text)\n",
    "        except:\n",
    "            company.append(\"-\")\n",
    "    for j in skil:\n",
    "        try:\n",
    "            skills.append(j.text)\n",
    "        except:\n",
    "            skills.append(\"-\")\n",
    "    for j in loc:\n",
    "        try:\n",
    "            location.append(j.text)\n",
    "        except:\n",
    "            location.append(\"-\")\n",
    "            \n",
    "# Data Frame\n",
    "\n",
    "Recruiter = pd.DataFrame(index = list(range(1,26)))\n",
    "Recruiter[\"Name\"] = name[:25]\n",
    "Recruiter[\"Designation\"] = designation[:25]\n",
    "Recruiter[\"Company\"] = company[:25]\n",
    "Recruiter[\"Skills\"] = skills[:25]\n",
    "Recruiter[\"Location\"] = location[:25]\n",
    "\n",
    "Recruiter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6898aab7",
   "metadata": {},
   "source": [
    "8. Scrape the details of Highest selling novels.\n",
    "Url = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey\u0002compare/\n",
    "You have to find the following details:\n",
    "\n",
    "A) Book name\n",
    "\n",
    "B) Author name\n",
    "\n",
    "C) Volumes sold\n",
    "\n",
    "D) Publisher\n",
    "\n",
    "E) Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c5c2a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7fcc8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp/ipykernel_11104/2845483631.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver=webdriver.Chrome(r\"C:\\Users\\USER\\Downloads\\chromedriver_win32 (3)\\chromedriver.exe\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<selenium.webdriver.chrome.webdriver.WebDriver (session=\"ec4d1fecb9a6bc5e831233572fd5ea98\")>\n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\USER\\Downloads\\chromedriver_win32 (3)\\chromedriver.exe\")\n",
    "print(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78f67953",
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare'\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba781c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all liabraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7f051d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "book=[]\n",
    "for i in driver.find_elements(By.XPATH,'//tbody//tr//td[2]'):\n",
    "    book.append(i.text)\n",
    "    \n",
    "author=[]\n",
    "for i in driver.find_elements(By.XPATH,'//tbody//tr//td[3]'):\n",
    "    author.append(i.text)\n",
    "    \n",
    "volume=[]\n",
    "for i in driver.find_elements(By.XPATH,'//tbody//tr//td[4]'):\n",
    "    volume.append(i.text)\n",
    "    \n",
    "publisher=[]\n",
    "for i in driver.find_elements(By.XPATH,'//tbody//tr//td[5]'):\n",
    "    publisher.append(i.text)\n",
    "    \n",
    "genre=[]\n",
    "for i in driver.find_elements(By.XPATH,'//tbody//tr//td[6]'):\n",
    "    genre.append(i.text)\n",
    "    \n",
    "que8=pd.DataFrame({'Book Name':book,'Author Name':author,'Volume Sales':volume,'Publisher':publisher,'Genre':genre})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b0b737",
   "metadata": {},
   "source": [
    "9. Scrape the details most watched tv series of all time from imdb.com. Url = https://www.imdb.com/list/ls095964455/\n",
    "You have to find the following details: A) Name B) Year span C) Genre D) Run time E) Ratings F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1ebe1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.imdb.com/list/ls095964455/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f5f6118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Year Span</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Game of Thrones</td>\n",
       "      <td>(2011–2019)</td>\n",
       "      <td>Action, Adventure, Drama</td>\n",
       "      <td>57 min</td>\n",
       "      <td>9.2</td>\n",
       "      <td>2,078,524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stranger Things</td>\n",
       "      <td>(2016– )</td>\n",
       "      <td>Drama, Fantasy, Horror</td>\n",
       "      <td>51 min</td>\n",
       "      <td>8.7</td>\n",
       "      <td>1,168,805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Walking Dead</td>\n",
       "      <td>(2010–2022)</td>\n",
       "      <td>Drama, Horror, Thriller</td>\n",
       "      <td>44 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>979,416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13 Reasons Why</td>\n",
       "      <td>(2017–2020)</td>\n",
       "      <td>Drama, Mystery, Thriller</td>\n",
       "      <td>60 min</td>\n",
       "      <td>7.5</td>\n",
       "      <td>291,543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The 100</td>\n",
       "      <td>(2014–2020)</td>\n",
       "      <td>Drama, Mystery, Sci-Fi</td>\n",
       "      <td>43 min</td>\n",
       "      <td>7.6</td>\n",
       "      <td>250,600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Reign</td>\n",
       "      <td>(2013–2017)</td>\n",
       "      <td>Drama</td>\n",
       "      <td>42 min</td>\n",
       "      <td>7.4</td>\n",
       "      <td>49,970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>A Series of Unfortunate Events</td>\n",
       "      <td>(2017–2019)</td>\n",
       "      <td>Adventure, Comedy, Drama</td>\n",
       "      <td>50 min</td>\n",
       "      <td>7.8</td>\n",
       "      <td>61,102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Criminal Minds</td>\n",
       "      <td>(2005–2020)</td>\n",
       "      <td>Crime, Drama, Mystery</td>\n",
       "      <td>42 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>196,618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Scream: The TV Series</td>\n",
       "      <td>(2015–2019)</td>\n",
       "      <td>Comedy, Crime, Drama</td>\n",
       "      <td>45 min</td>\n",
       "      <td>7.1</td>\n",
       "      <td>41,424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>The Haunting of Hill House</td>\n",
       "      <td>(2018)</td>\n",
       "      <td>Drama, Horror, Mystery</td>\n",
       "      <td>572 min</td>\n",
       "      <td>8.6</td>\n",
       "      <td>242,781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Name    Year Span                     Genre  \\\n",
       "0                  Game of Thrones  (2011–2019)  Action, Adventure, Drama   \n",
       "1                  Stranger Things     (2016– )    Drama, Fantasy, Horror   \n",
       "2                 The Walking Dead  (2010–2022)   Drama, Horror, Thriller   \n",
       "3                   13 Reasons Why  (2017–2020)  Drama, Mystery, Thriller   \n",
       "4                          The 100  (2014–2020)    Drama, Mystery, Sci-Fi   \n",
       "..                             ...          ...                       ...   \n",
       "95                           Reign  (2013–2017)                     Drama   \n",
       "96  A Series of Unfortunate Events  (2017–2019)  Adventure, Comedy, Drama   \n",
       "97                  Criminal Minds  (2005–2020)     Crime, Drama, Mystery   \n",
       "98           Scream: The TV Series  (2015–2019)      Comedy, Crime, Drama   \n",
       "99      The Haunting of Hill House       (2018)    Drama, Horror, Mystery   \n",
       "\n",
       "    Runtime Rating      Votes  \n",
       "0    57 min    9.2  2,078,524  \n",
       "1    51 min    8.7  1,168,805  \n",
       "2    44 min    8.1    979,416  \n",
       "3    60 min    7.5    291,543  \n",
       "4    43 min    7.6    250,600  \n",
       "..      ...    ...        ...  \n",
       "95   42 min    7.4     49,970  \n",
       "96   50 min    7.8     61,102  \n",
       "97   42 min    8.1    196,618  \n",
       "98   45 min    7.1     41,424  \n",
       "99  572 min    8.6    242,781  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name=[]\n",
    "for i in driver.find_elements(By.XPATH,'//h3//a'):\n",
    "    name.append(i.text)\n",
    "\n",
    "year=[]\n",
    "for i in driver.find_elements(By.XPATH,'//h3//span[@class=\"lister-item-year text-muted unbold\"]'):\n",
    "    year.append(i.text)\n",
    "    \n",
    "genre=[]\n",
    "for i in driver.find_elements(By.XPATH,'//p//span[@class=\"genre\"]'):\n",
    "    genre.append(i.text)\n",
    "    \n",
    "runtime=[]\n",
    "for i in driver.find_elements(By.XPATH,'//p//span[@class=\"runtime\"]'):\n",
    "    runtime.append(i.text)\n",
    "    \n",
    "rating=[]\n",
    "for i in driver.find_elements(By.XPATH,'//div[@class=\"ipl-rating-star small\"]//span[@class=\"ipl-rating-star__rating\"]'):\n",
    "    rating.append(i.text)\n",
    "    \n",
    "vote=[]\n",
    "for i in driver.find_elements(By.XPATH,'//span[@name=\"nv\"]'):\n",
    "    vote.append(i.text)\n",
    "    \n",
    "que9=pd.DataFrame({'Name':name,'Year Span':year,'Genre':genre,'Runtime':runtime,'Rating':rating,'Votes':vote})\n",
    "que9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a31954",
   "metadata": {},
   "source": [
    "10. Details of Datasets from UCI machine learning repositories. Url = https://archive.ics.uci.edu/\n",
    "You have to find the following details: A) Dataset name B) Data type C) Task D) Attribute type E) No of instances F) No of attribute G) Year\n",
    "\n",
    "Note: - from the home page you have to go to the ShowAllDataset page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b16c3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://archive.ics.uci.edu/'\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6519753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all=driver.find_element(By.XPATH,'//span[@class=\"whitetext\"]//b')\n",
    "all.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "501c20ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Default Task</th>\n",
       "      <th>Attribute Type</th>\n",
       "      <th>Instances</th>\n",
       "      <th>Attributes</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abalone</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer, Real</td>\n",
       "      <td>4177</td>\n",
       "      <td>8</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adult</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer</td>\n",
       "      <td>48842</td>\n",
       "      <td>14</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Annealing</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer, Real</td>\n",
       "      <td>798</td>\n",
       "      <td>38</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anonymous Microsoft Web Data</td>\n",
       "      <td></td>\n",
       "      <td>Recommender-Systems</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>37711</td>\n",
       "      <td>294</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arrhythmia</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer, Real</td>\n",
       "      <td>452</td>\n",
       "      <td>279</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>Influenza outbreak event prediction via Twitte...</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Integer, Real</td>\n",
       "      <td>75840</td>\n",
       "      <td>525</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>Turkish Music Emotion Dataset</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Integer, Real</td>\n",
       "      <td>400</td>\n",
       "      <td>50</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>Maternal Health Risk Data Set</td>\n",
       "      <td></td>\n",
       "      <td>Classification</td>\n",
       "      <td></td>\n",
       "      <td>1014</td>\n",
       "      <td>7</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>Room Occupancy Estimation</td>\n",
       "      <td>Multivariate, Time-Series</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real</td>\n",
       "      <td>10129</td>\n",
       "      <td>16</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>Image Recognition Task Execution Times in Mobi...</td>\n",
       "      <td>Univariate</td>\n",
       "      <td>Regression</td>\n",
       "      <td>Real</td>\n",
       "      <td>4000</td>\n",
       "      <td>2</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>622 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Name  \\\n",
       "0                                              Abalone   \n",
       "1                                                Adult   \n",
       "2                                            Annealing   \n",
       "3                         Anonymous Microsoft Web Data   \n",
       "4                                           Arrhythmia   \n",
       "..                                                 ...   \n",
       "617  Influenza outbreak event prediction via Twitte...   \n",
       "618                      Turkish Music Emotion Dataset   \n",
       "619                      Maternal Health Risk Data Set   \n",
       "620                          Room Occupancy Estimation   \n",
       "621  Image Recognition Task Execution Times in Mobi...   \n",
       "\n",
       "                      Data Type          Default Task  \\\n",
       "0                 Multivariate        Classification    \n",
       "1                 Multivariate        Classification    \n",
       "2                 Multivariate        Classification    \n",
       "3                                Recommender-Systems    \n",
       "4                 Multivariate        Classification    \n",
       "..                          ...                   ...   \n",
       "617               Multivariate        Classification    \n",
       "618               Multivariate        Classification    \n",
       "619                                   Classification    \n",
       "620  Multivariate, Time-Series        Classification    \n",
       "621                 Univariate            Regression    \n",
       "\n",
       "                  Attribute Type Instances Attributes   Year  \n",
       "0    Categorical, Integer, Real      4177          8   1995   \n",
       "1          Categorical, Integer     48842         14   1996   \n",
       "2    Categorical, Integer, Real       798         38          \n",
       "3                   Categorical     37711        294   1998   \n",
       "4    Categorical, Integer, Real       452        279   1998   \n",
       "..                           ...       ...        ...    ...  \n",
       "617               Integer, Real     75840        525   2020   \n",
       "618               Integer, Real       400         50   2020   \n",
       "619                                  1014          7   2020   \n",
       "620                        Real     10129         16   2021   \n",
       "621                        Real      4000          2   2021   \n",
       "\n",
       "[622 rows x 7 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name=[]\n",
    "for i in driver.find_elements(By.XPATH,'//b//a'):\n",
    "    name.append(i.text)\n",
    "    \n",
    "tyep=[]\n",
    "for i in driver.find_elements(By.XPATH,'/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[2]'):\n",
    "    tyep.append(i.text)\n",
    "tyep.pop(0)\n",
    "\n",
    "task=[]\n",
    "for i in driver.find_elements(By.XPATH,'/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[3]'):\n",
    "    task.append(i.text)\n",
    "task.pop(0)\n",
    "\n",
    "att_type=[]\n",
    "for i in driver.find_elements(By.XPATH,'/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[4]'):\n",
    "    att_type.append(i.text)\n",
    "att_type.pop(0)\n",
    "\n",
    "inst=[]\n",
    "for i in driver.find_elements(By.XPATH,'/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[5]'):\n",
    "    inst.append(i.text)\n",
    "inst.pop(0)\n",
    "\n",
    "att=[]\n",
    "for i in driver.find_elements(By.XPATH,'/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[6]'):\n",
    "    att.append(i.text)\n",
    "att.pop(0)\n",
    "\n",
    "year=[]\n",
    "for i in driver.find_elements(By.XPATH,'/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[7]'):\n",
    "    year.append(i.text)\n",
    "year.pop(0)\n",
    "\n",
    "que10=pd.DataFrame({'Name':name,'Data Type':tyep,'Default Task':task,'Attribute Type':att_type,'Instances':inst,'Attributes':att,'Year':year})\n",
    "que10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5a164c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50889cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
